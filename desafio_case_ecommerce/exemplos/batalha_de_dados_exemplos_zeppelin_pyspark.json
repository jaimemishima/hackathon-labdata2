{"paragraphs":[{"title":"Exibir databases","text":"%pyspark\nspark.sql(\"show databases\").show()\n","user":"anonymous","dateUpdated":"2019-04-12T23:20:43+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1555111243239_1381511461","id":"20181208-134230_1861185261","dateCreated":"2019-04-12T23:20:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:188"},{"title":"Acessar database e exibir tabelas","text":"%pyspark\nspark.sql(\"use sessions_ecommerce\")\nspark.sql(\"show tables\").show()","user":"anonymous","dateUpdated":"2019-04-12T23:20:43+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1555111243240_1075668992","id":"20181208-134259_1561086742","dateCreated":"2019-04-12T23:20:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:189"},{"title":"selecionar tabela rawdata","text":"%pyspark\ndf = spark.sql(\"select * from sessions_ecommerce.rawdata\")\ndf.show()","user":"anonymous","dateUpdated":"2019-04-12T23:20:43+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1555111243240_903124568","id":"20181208-134500_546755517","dateCreated":"2019-04-12T23:20:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:190"},{"title":"Também é possível acessar diretamente o S3 em vez de ler tabelas","text":"%pyspark\n# substitua o <bucket-name> pelo nome do seu bucket. Coloque ano, mes, dia e hora que exista no seu bucket s3. \n# df = spark.read.json(\"s3://<bucket-name>/rawdata/ano/mes/dia/hora/\")\ndf = spark.read.json(\"s3://samuel-schmidt-demo-ecomm-bucket-stream/rawdata/2018/12/08/05/\")\ndf.show()","user":"anonymous","dateUpdated":"2019-04-12T23:20:43+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1555111243240_275326939","id":"20181208-134540_1669931981","dateCreated":"2019-04-12T23:20:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:191"},{"text":"%pyspark\n# substitua o <bucket-name> pelo nome do seu bucket. Coloque ano, mes, dia e hora que exista no seu bucket s3\n# df = spark.read.csv(\"s3://<bucket-name>/aggregated/ano/mes/dia/hora/\")\ndf2 = spark.read.csv(\"s3://samuel-schmidt-demo-ecomm-bucket-stream/aggregated/2018/12/08/05/\")\ndf2.show()","user":"anonymous","dateUpdated":"2019-04-12T23:20:43+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1555111243241_-635573131","id":"20181208-140532_192880319","dateCreated":"2019-04-12T23:20:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:192"},{"text":"%pyspark\nDESTINO='s3://bucket-novo/destino'\n    df_final.write \\\n        .format(\"parquet\") \\\n        .option(\"compression\", \"snappy\") \\\n        .mode(\"append\") \\\n        .save(path=DESTINO)\n","user":"anonymous","dateUpdated":"2019-04-12T23:24:11+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1555111373331_-1303647876","id":"20190412-232253_1471290996","dateCreated":"2019-04-12T23:22:53+0000","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:714","dateFinished":"2019-04-12T23:23:48+0000","dateStarted":"2019-04-12T23:23:48+0000","title":"Gravar o dataframe em formato parquet em um bucket de destino","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-4274690422800511081.py\", line 364, in <module>\n    code = compile('\\n'.join(stmts), '<stdin>', 'exec', ast.PyCF_ONLY_AST, 1)\n  File \"<stdin>\", line 2\n    df_final.write \\\n    ^\nIndentationError: unexpected indent\n"}]}},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2019-04-12T23:23:48+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1555111428005_970557456","id":"20190412-232348_380602243","dateCreated":"2019-04-12T23:23:48+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:839"}],"name":"batalha_de_dados_exemplos_zeppelin_pyspark","id":"2E9AHJ84F","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}